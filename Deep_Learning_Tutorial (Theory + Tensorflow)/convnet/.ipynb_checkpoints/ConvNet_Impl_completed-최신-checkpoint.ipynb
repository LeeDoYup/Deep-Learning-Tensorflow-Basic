{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Network를 통한 MNIST 예측\n",
    "\n",
    "MNIST 데이터셋은 ConvNet의 성능을 평가하기 위한 가장 대표적인 벤치마크 입니다.\n",
    "0~9 사이의 숫자가 28 * 28 이미지에 입력되어 있으며 (input, X)\n",
    "이에 대한 분류 정답은 자연스럽게 0~9 사이의 값입니다.\n",
    "\n",
    "일반적으로 머신러닝에서 Classification 문제를 풀 때는 Labeled Data Y를 \n",
    "\n",
    "\"One-hot encoding\"하여 사용합니다.\n",
    "\n",
    "예를 들어, 정답이 0~9 사이의 정수형 숫자라면\n",
    "\n",
    "길이가 10인 벡터를 만들어서,\n",
    "\n",
    "정답이 9인 경우에는 [0,0,0,0,0,0,0,0,0,1]\n",
    "정답이 1인 경우에는 [1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "이런 방식으로 해당 순서의 원소는 1, 그렇지 않은 원소는 모든 0으로 벡터를 표기합니다.\n",
    "\n",
    "(정답인 레이블에 해당하는 인덱스 값만 1이기 때문에 one-hot 이라고 부릅니다!)\n",
    "\n",
    "이제부터 딥러닝 모델의 가장 대표적인 모델 중 하나인 Convolutional Neural Network를 통해\n",
    "\n",
    "MNIST 이미지를 분류를 해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "#Tensorflow log management: '1':info, '2':warning, '3': error\n",
    "\n",
    "import time \n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#Tutorials have mnist data files and queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 읽기\n",
    "\n",
    "MNIST 데이터의 경우, 매우 대표적인 벤치마크 데이터이기 때문에 tensorflow 설치 시 자동으로 설치가 되어 있습니다.\n",
    "\n",
    "또한, 이 데이터를 편하게 다룰 수 있도록 자체적으로 메소드들이 구현되어 있어 쉽게 사용할 수 있습니다.\n",
    "\n",
    "아래 함수를 이용하면 MNIST 데이터를 one-hot encoding한 레이블과 함께 바로 사용하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/data/mnist\", one_hot=True)\n",
    "#MNIST images' has 28*28=764 features. Gray Image: Depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2모델 생성\n",
    "\n",
    "### Placeholder 선언\n",
    "\n",
    "본격적으로 Convolutional Neural Network 모델을 생성하도록 하겠습니다.\n",
    "\n",
    "우리가 만들 모델은\n",
    "\n",
    "Input(784) - Conv1(5x5, 32) - Pool1 (2x2) - Conv2(5x5, 64) - Pool2 (2x2) - FC(1024) - Output(10)\n",
    "\n",
    "의 구조를 가진 딥러닝 모델입니다.\n",
    "\n",
    "아래는 Input Layer와 정답 Y를 Placeholder로 선언한 것입니다.\n",
    "\n",
    "이번에 우리가 Dropout이라는 모델 일반화 방법을 함께 사용할 것이기 때문에, 이를 위한 dropout 비율의 Placeholder도 선언하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout') #Dropout Rate\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1-Pool1 Layer 구축\n",
    "\n",
    "아래는 Convolutional Layer와 Pooling Layer를 순차적으로 구성하는 부분입니다.\n",
    "\n",
    "우선 tf.nn.conv2d의 Input은 기본적으로 4차원의 텐서를 받습니다.\n",
    "(디폴트: Batch size * Height * Width * Depth)\n",
    "\n",
    "즉, 기존 Flatten 되어 있는 MNIST 이미지를 2차원으로 다시 재구성해주어야합니다.\n",
    "\n",
    "이후 5 * 5 모양의 필터를 선언하기 이해, [5,5,이전 layer depth, 이 후 layer depth] 모양의 변수를 선언하도록 합니다.\n",
    "\n",
    "ConvNet에서 학습의 대상은 바로 필터이기 때문에 반드시 변수로 선언해주어야 합니다!\n",
    "\n",
    "Pooling Layer는 유사한 방식으로 tf.nn.max_pool을 사용하면 쉽게 구현하실 수 있습니다.\n",
    "\n",
    "\n",
    "다른 방법으로 high-level API를 사용하여 ConvNet을 구현할 수 있습니다.\n",
    "\n",
    "layers.conv2d(input, channel, kernel_size, stride=(1,1), input_channel, activation, padding ...)를 입력하면 별도의 변수 선언 등의 과정 없이 손쉽게 conv2d를 구현하실 수 있습니다.\n",
    "\n",
    "하지만 처음 사용할 경우, high-level API 보다는 직접 low-level API를 하나하나 사용해보는 것을 매우 권장합니다.\n",
    "\n",
    "(새로운 모델이 나올 경우, 대부분의 모델은 low-level API를 통해 구현되어 있기 때문!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1]) \n",
    "    \n",
    "    kernel = tf.get_variable('kernel', [5, 5, 1, 32], \n",
    "                            initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    biases = tf.get_variable('biases', [32],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 28 x 28 x 32\n",
    "    # conv1 = layers.conv2d(images, 32, 5, 1, activation_fn=tf.nn.relu, padding='SAME')\n",
    "\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2-Pool2 Layer 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pool1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1b8ce5e39e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     biases = tf.get_variable('biases', [64],\n\u001b[1;32m      6\u001b[0m                         initializer=tf.random_normal_initializer())\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pool1' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC Layer 구축\n",
    "\n",
    "FC Layer를 구현은 이전 예시들과 동일합니다.\n",
    "\n",
    "다만 현재 convolutional layer와 pooling layer의 input, output의 모양이 4차원이므로 \n",
    "\n",
    "이를 2차원으로 바꾸어준 후 계산해야 합니다.\n",
    "\n",
    "[Batch_Size, Height, Width, Depth] --> [Batch_Size, Height * Width * Depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc') as scope:\n",
    "    input_features = 7 * 7 * 64\n",
    "    w = tf.get_variable('weights', [input_features, 1024],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [1024],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pool2 = layers.flatten(pool2)\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    # fc = layers.fully_connected(pool2, 1024, tf.nn.relu)\n",
    "\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer 구축\n",
    "\n",
    "Output Layer는 Softmax 함수를 통해 최종적으로 우리가 도출한 feature의 계산이 각 class (0~9)에 속할 확률을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    w = tf.get_variable('weights', [1024, N_CLASSES],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    logits = tf.matmul(fc, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, Tensorboard를 위한 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(entropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer 선언 (with global step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "train = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Session, Initialization, FileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to visualize using TensorBoard\n",
    "writer = tf.summary.FileWriter('./graphs/convnet', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Point 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('./checkpoints')\n",
    "os.mkdir('./checkpoints/convnet_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('./checkpoints/convnet_mnist/checkpoint'))\n",
    "# if that checkpoint exists, restore from checkpoint\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 2234.2\n",
      "Average loss at step 20: 920.0\n",
      "Average loss at step 30: 537.3\n",
      "Average loss at step 40: 351.8\n",
      "Average loss at step 50: 244.7\n",
      "Average loss at step 60: 237.8\n",
      "Average loss at step 70: 199.4\n",
      "Average loss at step 80: 220.2\n",
      "Average loss at step 90: 158.0\n",
      "Average loss at step 100: 146.7\n",
      "Average loss at step 110:  98.2\n",
      "Average loss at step 120: 113.0\n",
      "Average loss at step 130:  89.7\n",
      "Average loss at step 140:  86.5\n",
      "Average loss at step 150:  71.6\n",
      "Average loss at step 160:  86.4\n",
      "Average loss at step 170:  80.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b33bc7416972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     _, loss_batch, summary = sess.run([train, loss, summary_op], \n\u001b[0;32m---> 10\u001b[0;31m                                       feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_step = sess.run(global_step)\n",
    "\n",
    "start_time = time.time()\n",
    "n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "total_loss = 0.0\n",
    "for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "    X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "    _, loss_batch, summary = sess.run([train, loss, summary_op], \n",
    "                                      feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "    writer.add_summary(summary, global_step=index)\n",
    "    total_loss += loss_batch\n",
    "    if (index + 1) % SKIP_STEP == 0:\n",
    "        print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "        total_loss = 0.0\n",
    "        saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "\n",
    "print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "print(\"Total time: {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the model\n",
    "n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "total_correct_preds = 0\n",
    "\n",
    "for i in range(n_batches):\n",
    "    X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "    _, loss_batch, logits_batch = sess.run([train, loss, logits], \n",
    "                                    feed_dict={X: X_batch, Y:Y_batch, dropout: 1.0})\n",
    "    \n",
    "    preds = tf.nn.softmax(logits_batch)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, axis=1), tf.argmax(Y_batch, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    total_correct_preds += sess.run(accuracy)   \n",
    "\n",
    "total_test_num = int(mnist.test.num_examples/BATCH_SIZE)*BATCH_SIZE\n",
    "print(\"Accuracy {0}\".format(total_correct_preds/total_test_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
