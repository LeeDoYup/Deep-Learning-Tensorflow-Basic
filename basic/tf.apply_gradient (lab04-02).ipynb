{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 본 예제는 optimizer 사용에 있어 사용자가 정의한 gradient를 직접 학습에 적용하는 방법을 익히는 데 초점을 둠.\n",
    "전반적인 내용은 모두 일치하며 아래 함수가 주된 내용임\n",
    "\n",
    "* optimizer.compute_gradients(loss, vars) : vars에 포함된 각 variable에 대하여 loss의 gradient 값을 리스트로 리턴함\n",
    "* optimizer.apply_gradient(grad) : 사용자가 직접 정의한 gradient 값인 grad를 적용하여 파라미터 업데이트를 진행. minimize()의 second part.\n",
    "* tf.clip_by_value(grad, min, max) : 입력된 grad값과 min, max 범위를 고려하여 grad 값이 범위 밖에 있을 경우, min 또는 max 값으로 대체\n",
    "\n",
    "\n",
    "### 주의할 점\n",
    "compute_gradients 함수는 (gradient 값, 대상 variable) pair로 이루어진 리스트를 반환한다.\n",
    "\n",
    "따라서 사용자가 직접 gradient를 tf.apply_gradients를 통해 지정해 줄 때도, 위의 형식을 맞춰주어야한다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "# This is optional\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다양한 gradient 직접 적용\n",
    "아래 부분이 optimizer의 내부 함수를 통해 각 변수별 gradient 값을 계산하고,\n",
    "\n",
    "계산한 결과를 직접 apply_gradient 를 통해 반영하여 학습하는 부분이다.\n",
    "\n",
    "apply_gradient를 실행할 경우, 적용한 gradient에 따라 파라미터 업데이트 (학습)이 진행되므로 별도로 optimizer.minimize(cost)를 실행해줄 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor 'gradients_2/mul_grad/tuple/control_dependency_1:0' shape=() dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x10b373a50>)]\n",
      "(0, [37.333332, 5.0, [(1.0, 5.0)]])\n",
      "(1, [37.239994, 4.9899998, [(1.0, 4.9899998)]])\n",
      "(2, [37.146664, 4.9799995, [(1.0, 4.9799995)]])\n",
      "(3, [37.053329, 4.9699993, [(1.0, 4.9699993)]])\n",
      "(4, [36.959991, 4.9599991, [(1.0, 4.9599991)]])\n",
      "(5, [36.866653, 4.9499989, [(1.0, 4.9499989)]])\n",
      "(6, [36.773319, 4.9399986, [(1.0, 4.9399986)]])\n",
      "(7, [36.679989, 4.9299984, [(1.0, 4.9299984)]])\n",
      "(8, [36.586651, 4.9199982, [(1.0, 4.9199982)]])\n",
      "(9, [36.493313, 4.9099979, [(1.0, 4.9099979)]])\n",
      "(10, [36.399979, 4.8999977, [(1.0, 4.8999977)]])\n",
      "(11, [36.306644, 4.8899975, [(1.0, 4.8899975)]])\n",
      "(12, [36.213306, 4.8799973, [(1.0, 4.8799973)]])\n",
      "(13, [36.119968, 4.869997, [(1.0, 4.869997)]])\n",
      "(14, [36.026638, 4.8599968, [(1.0, 4.8599968)]])\n",
      "(15, [35.933304, 4.8499966, [(1.0, 4.8499966)]])\n",
      "(16, [35.839966, 4.8399963, [(1.0, 4.8399963)]])\n",
      "(17, [35.746628, 4.8299961, [(1.0, 4.8299961)]])\n",
      "(18, [35.653294, 4.8199959, [(1.0, 4.8199959)]])\n",
      "(19, [35.559963, 4.8099957, [(1.0, 4.8099957)]])\n",
      "(20, [35.466625, 4.7999954, [(1.0, 4.7999954)]])\n",
      "(21, [35.373287, 4.7899952, [(1.0, 4.7899952)]])\n",
      "(22, [35.279953, 4.779995, [(1.0, 4.779995)]])\n",
      "(23, [35.186619, 4.7699947, [(1.0, 4.7699947)]])\n",
      "(24, [35.093281, 4.7599945, [(1.0, 4.7599945)]])\n",
      "(25, [34.999943, 4.7499943, [(1.0, 4.7499943)]])\n",
      "(26, [34.906612, 4.739994, [(1.0, 4.739994)]])\n",
      "(27, [34.813278, 4.7299938, [(1.0, 4.7299938)]])\n",
      "(28, [34.71994, 4.7199936, [(1.0, 4.7199936)]])\n",
      "(29, [34.626602, 4.7099934, [(1.0, 4.7099934)]])\n",
      "(30, [34.533268, 4.6999931, [(1.0, 4.6999931)]])\n",
      "(31, [34.439938, 4.6899929, [(1.0, 4.6899929)]])\n",
      "(32, [34.3466, 4.6799927, [(1.0, 4.6799927)]])\n",
      "(33, [34.253262, 4.6699924, [(1.0, 4.6699924)]])\n",
      "(34, [34.159927, 4.6599922, [(1.0, 4.6599922)]])\n",
      "(35, [34.066593, 4.649992, [(1.0, 4.649992)]])\n",
      "(36, [33.973255, 4.6399918, [(1.0, 4.6399918)]])\n",
      "(37, [33.879917, 4.6299915, [(1.0, 4.6299915)]])\n",
      "(38, [33.786587, 4.6199913, [(1.0, 4.6199913)]])\n",
      "(39, [33.693253, 4.6099911, [(1.0, 4.6099911)]])\n",
      "(40, [33.599915, 4.5999908, [(1.0, 4.5999908)]])\n",
      "(41, [33.506577, 4.5899906, [(1.0, 4.5899906)]])\n",
      "(42, [33.413242, 4.5799904, [(1.0, 4.5799904)]])\n",
      "(43, [33.319912, 4.5699902, [(1.0, 4.5699902)]])\n",
      "(44, [33.226574, 4.5599899, [(1.0, 4.5599899)]])\n",
      "(45, [33.133236, 4.5499897, [(1.0, 4.5499897)]])\n",
      "(46, [33.039902, 4.5399895, [(1.0, 4.5399895)]])\n",
      "(47, [32.946568, 4.5299892, [(1.0, 4.5299892)]])\n",
      "(48, [32.85323, 4.519989, [(1.0, 4.519989)]])\n",
      "(49, [32.759895, 4.5099888, [(1.0, 4.5099888)]])\n",
      "(50, [32.666561, 4.4999886, [(1.0, 4.4999886)]])\n",
      "(51, [32.573223, 4.4899883, [(1.0, 4.4899883)]])\n",
      "(52, [32.479889, 4.4799881, [(1.0, 4.4799881)]])\n",
      "(53, [32.386555, 4.4699879, [(1.0, 4.4699879)]])\n",
      "(54, [32.293217, 4.4599876, [(1.0, 4.4599876)]])\n",
      "(55, [32.199883, 4.4499874, [(1.0, 4.4499874)]])\n",
      "(56, [32.106548, 4.4399872, [(1.0, 4.4399872)]])\n",
      "(57, [32.01321, 4.429987, [(1.0, 4.429987)]])\n",
      "(58, [31.919876, 4.4199867, [(1.0, 4.4199867)]])\n",
      "(59, [31.82654, 4.4099865, [(1.0, 4.4099865)]])\n",
      "(60, [31.733206, 4.3999863, [(1.0, 4.3999863)]])\n",
      "(61, [31.63987, 4.389986, [(1.0, 4.389986)]])\n",
      "(62, [31.546534, 4.3799858, [(1.0, 4.3799858)]])\n",
      "(63, [31.453199, 4.3699856, [(1.0, 4.3699856)]])\n",
      "(64, [31.359863, 4.3599854, [(1.0, 4.3599854)]])\n",
      "(65, [31.266527, 4.3499851, [(1.0, 4.3499851)]])\n",
      "(66, [31.173193, 4.3399849, [(1.0, 4.3399849)]])\n",
      "(67, [31.079857, 4.3299847, [(1.0, 4.3299847)]])\n",
      "(68, [30.986521, 4.3199844, [(1.0, 4.3199844)]])\n",
      "(69, [30.893187, 4.3099842, [(1.0, 4.3099842)]])\n",
      "(70, [30.79985, 4.299984, [(1.0, 4.299984)]])\n",
      "(71, [30.706514, 4.2899837, [(1.0, 4.2899837)]])\n",
      "(72, [30.61318, 4.2799835, [(1.0, 4.2799835)]])\n",
      "(73, [30.519844, 4.2699833, [(1.0, 4.2699833)]])\n",
      "(74, [30.426508, 4.2599831, [(1.0, 4.2599831)]])\n",
      "(75, [30.333174, 4.2499828, [(1.0, 4.2499828)]])\n",
      "(76, [30.239838, 4.2399826, [(1.0, 4.2399826)]])\n",
      "(77, [30.146502, 4.2299824, [(1.0, 4.2299824)]])\n",
      "(78, [30.053167, 4.2199821, [(1.0, 4.2199821)]])\n",
      "(79, [29.959831, 4.2099819, [(1.0, 4.2099819)]])\n",
      "(80, [29.866495, 4.1999817, [(1.0, 4.1999817)]])\n",
      "(81, [29.773161, 4.1899815, [(1.0, 4.1899815)]])\n",
      "(82, [29.679825, 4.1799812, [(1.0, 4.1799812)]])\n",
      "(83, [29.586489, 4.169981, [(1.0, 4.169981)]])\n",
      "(84, [29.493155, 4.1599808, [(1.0, 4.1599808)]])\n",
      "(85, [29.399818, 4.1499805, [(1.0, 4.1499805)]])\n",
      "(86, [29.306482, 4.1399803, [(1.0, 4.1399803)]])\n",
      "(87, [29.213148, 4.1299801, [(1.0, 4.1299801)]])\n",
      "(88, [29.119812, 4.1199799, [(1.0, 4.1199799)]])\n",
      "(89, [29.026476, 4.1099796, [(1.0, 4.1099796)]])\n",
      "(90, [28.933142, 4.0999794, [(1.0, 4.0999794)]])\n",
      "(91, [28.839806, 4.0899792, [(1.0, 4.0899792)]])\n",
      "(92, [28.746469, 4.0799789, [(1.0, 4.0799789)]])\n",
      "(93, [28.653135, 4.0699787, [(1.0, 4.0699787)]])\n",
      "(94, [28.559799, 4.0599785, [(1.0, 4.0599785)]])\n",
      "(95, [28.466463, 4.0499783, [(1.0, 4.0499783)]])\n",
      "(96, [28.373129, 4.039978, [(1.0, 4.039978)]])\n",
      "(97, [28.279793, 4.0299778, [(1.0, 4.0299778)]])\n",
      "(98, [28.186457, 4.0199776, [(1.0, 4.0199776)]])\n",
      "(99, [28.093122, 4.0099773, [(1.0, 4.0099773)]])\n"
     ]
    }
   ],
   "source": [
    "# Get gradients\n",
    "# gvs = optimizer.compute_gradients(cost, [W]) #원래 gradient 값을 계산하여 반환받는 부분\n",
    "print gvs\n",
    "# Optional: modify gradient if necessary\n",
    "gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs] # gradient의 범위를 지정했을 경우\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs) #이를 실행하면 파라미터 업데이트 까지 진행됨\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print(step, sess.run([gradient, W, gvs]))\n",
    "    sess.run(apply_gradients)\n",
    "    # Same as sess.run(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 보면, 우리가 직접 계산한 (manual gradient)의 값이 -1, +1 사이의 범위를 벗어나 실제 적용된 gradient 값은 모두 1임을 알 수 있다.\n",
    "\n",
    "실제 gradient 크기보다 훨씬 작은 값이 학습에 적용되었기 때문에 학습이 더딘 것을 볼 수 있다.\n",
    "\n",
    "하지만, 이러한 clipping 방법은 학습에 있어 gradient exploding 또는 vanishing 현상을 1차적으로 방지할 수 있는 가장 기본적인 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
